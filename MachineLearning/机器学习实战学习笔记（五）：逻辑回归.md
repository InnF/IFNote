# Logistic 回归

 > 假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称做回归
 > 利用 Logistic 回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类
 > “回归”一词源于最佳拟合，表示要找到最佳拟合参数集


 Logistic 回归的一般过程

 + 收集数据：采用任意方法收集数据
 + 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳
 + 分析数据：采用任意方法对数据进行分析
 + 训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数
 + 测试算法：一旦训练步骤完成，分类将会很快
 + 使用算法：首先需要输入一些数据，并将其转换成对应的结构化数值；接着基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后我们就可以在输出的类别上做一些其他分析工作


## 基于 Logistic 回归和 Sigmoid 函数的分类

 + 优点：计算代价不高，易于理解和实现
 + 缺点：容易欠拟合，分类精度可能不高
 + 适用数据类型：数值型和标称型数据


我们希望我们的函数能够对我们的输入进行分类，如果是两个类别的话，那么会输出 0 和 1。这种函数称为 海维塞德阶跃函数 (Heaviside step function)，或称为单位阶跃函数。但是这种函数存在问题就是，该函数在跳跃点上瞬间从 0 跳到了 1，这个瞬间跳跃的过程有时很难处理好。那么这时候使用 Sigmoid 函数，该函数具有相同的性质，并且数学上更易处理

<br />
<div align="center">
	<img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML/sigmoid.jpg?raw=true"/>
</div>
<br />

对 Sigmoid 函数而言，下面给出了两个不同坐标尺度下的两条曲线图

<br />
<div align="center">
	<img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML//Sigmoid_plot_1.png?raw=true"/>
	<img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML//Sigmoid_plot_2.png?raw=true"/>
</div>
<br />

当 x 为 0 时，Sigmoid 函数值为 0.5。随着 x 的增大，对应的 Sigmoid 值逼近于 1；而随着 x 的减少，Sigmoid 值逼近于 0。如果横坐标刻度足够大，Sigmoid 函数就像一个阶跃函数

因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数（Weight），然后所有的结果值相加，将这个总和带入 Sigmoid 函数，进而得到一个范围在 0 ~ 1 之间的数值。任何大于 0.5 的数据被归为 1 类，任何小于 0.5 的数据被归为 0 类。所以，Logistic 回归也可以被看成是一种概率估计。

## 基于最优化方法的最佳回归系数确定

现在问题就转变为了：最佳回归系数（Weight）是多少？如何确定它们的大小？

若 Sigmoid 函数的输入记为 z，那么就可以得到

<br />
<div align="center">
	<img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML/z_weight_input_method.jpg?raw=true"/>
</div>
<br />

如果采用向量的写法，则可以写成

<br />
<div align="center">
	<img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML/z_weight_input_method_2.jpg?raw=true"/>
</div>
<br />


## 梯度上升法

> 要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示：


<br />
<div align="center">
  <img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML/gradient_ascent_method.png?raw=true"/>
</div>
<br />

这个梯度意味着要沿 x 的方向移动 ![](./res/_x.png) ，沿 y 的方向移动 ![](./res/_y.png)。其中，函数 `f(x,y)` 必须要在待计算的点上有定义并且可微。

梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作 `α`。用向量来表示的话，梯度上升算法的迭代公式如下：

<br />
<div align="center">
	<img src="https://github.com/InnoFang/oh-my-study-notes/blob/image-hosting/Algo4ML/gradient_ascent_iteration_formula.png?raw=true"/>
</div>
<br />

该公式一直迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或算法达到某个可以允许的误差范围

> 与梯度上升算法相对应有一个**梯度下降算法**，与梯度上升迭代公式的区别就是把 `+` 换成了 `-`。
> 梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值


## 随机梯度上升法

梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理 100 个左右的数据时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。

一个改进方法是一次仅用一个样本点来更新回归系数，该方法称为**随机梯度上升算法那**。

由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与“在线学习”相对应，一次处理所有数据被称作是“批处理”

## 处理数据中的缺失值

数据中的缺失值是个非常棘手的问题。数据代价有时候十分昂贵，因为某些特征值的缺失而丢弃所有数据并不可取，所以必须采用一些方法来解决这个问题

 + 使用可用特征的均值来填补缺失值
 + 使用特殊值来填补缺失值，如 -1
 + 忽略有缺失值的版本
 + 使用相似样本的均值填补添补缺失值
 + 使用另外的机器学习算法预测缺失值

## Logistic 回归小结

Logistic 回归的目的是寻找一个非线性函数 Sigmoid 的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法
又可以简化成随机梯度上升算法

随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理运算。

机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用中的需求。现有一些解决方案，每种方案都各有优缺点。
 