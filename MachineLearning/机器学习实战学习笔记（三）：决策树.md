# 决策树

 k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解

 决策树的一个重要任务是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。

 ## 决策树的构造

 + 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
 + 缺点：可能产生过度匹配问题
 + 使用数据类型：标称型和数值型

 在构造决策树时，需要找到哪个特征在划分数据分类时起决定性作用，为了找到决定性特征，划分出最好的结果，我们需要评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程，直到所有具有相同类型的数据均在一个数据子集内


 决策树的一般流程
  + 收集数据：可以使用任何方法
  + 输入数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
  + 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
  + 训练算法：构造树的数据结构
  + 测试算法：使用经验树计算错误率
  + 使用算法：此步骤可以适用于任何监督学习算法那，而使用决策树可以更好地理解数据的内在含义


## 使用 ID3 算法划分数据集

### 算法介绍

> 该算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类

> 在信息论中，期望信息越小，那么信息增益就越大，从而纯度就越高。

> ID3算法的核心思想就是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分裂。

> 该算法采用自顶向下的贪婪搜索遍历可能的决策空间。

### 信息熵和信息增益

#### 信息熵(Entropy)

  **熵**在物理学中的概念表示一个系统的无序程度，而在信息论中则表示对不确定性的度量。道理是相似的，信息熵越低，则系统越有序，反之，信息熵越高，则系统越混乱。


  假如一个随机变量 X 的取值为 X={x1,x2,...,xn}，每一种取到的概率分别是 {p1,p2,...,pn}，那么 X 的熵定义为

  ![](http://images.cnitblog.com/blog/571227/201412/112112589313898.png)


  对于**分类系统**而言，将类别表示为 C，那么 C1，C2,...,Cn，每一个类别出现的概率就是 `P(C1),P(C2),...,P(Cn)`。n 表示类别总数。此时，分类系统的熵，即**信息熵**就可以表示为

  ![](http://images.cnitblog.com/blog/571227/201412/112123167755487.png)


#### 信息增益(Information gain)

  对每个特征而言，系统有它和没它时的信息量各是多少，两者的差值就是这个特征给系统带来的信息量。简单的说，信息增益指的是在进行属性划分前后信息的差值

  举个例子，以**是否买电脑**为例，下面给出一个表，学习目标是**买**或者**不买**

| id   | 年龄   | 收入   | 学生   | 信用率  | 类别：是否买电脑 |
| ---- | ---- | ---- | ---- | ---- | -------- |
| 1    | 年轻   | 高    | 否    | 一般   | 否        |
| 2    | 年轻   | 高    | 否    | 极好   | 否        |
| 3    | 中年   | 高    | 否    | 一般   | 是        |
| 4    | 年长   | 中    | 否    | 一般   | 是        |
| 5    | 年长   | 低    | 是    | 一般   | 是        |
| 6    | 年长   | 低    | 是    | 极好   | 否        |
| 7    | 中年   | 低    | 是    | 极好   | 是        |
| 8    | 年轻   | 中    | 否    | 一般   | 否        |
| 9    | 年轻   | 低    | 是    | 一般   | 是        |
| 10   | 年长   | 中    | 是    | 一般   | 是        |
| 11   | 年轻   | 中    | 是    | 极好   | 是        |
| 12   | 中年   | 中    | 否    | 极好   | 是        |
| 13   | 中年   | 高    | 是    | 一般   | 是        |
| 14   | 年长   | 中    | 否    | 极好   | 否        |

一共 14 个样例，包括 5 个负例和 9 个正例。那么信息熵计算如下

![](http://images.cnitblog.com/blog/571227/201412/121404563217970.png)

接下来，我们假设按年龄（age）进行分类，其年龄有三个类别：年轻（youth）、中年（middle age）、年长（senior），那么对应的年龄与是否购买电脑的关系大致如下：

``` 
			age	
  ___________|___________     
  |          |          |
youth	middle age 	  senior
  |          |          |
-----      -----      -----  
  no        yes 	   yes
  no        yes 	   yes
  no        yes 	   no
  yes       yes 	   yes
  yes       yes 	   no
```

划分后，各个分支的信息熵计算如下

![](https://ws1.sinaimg.cn/large/0067fiZ7ly1fn2lrbqvyrj30n301qaa3.jpg)

![](https://ws1.sinaimg.cn/large/0067fiZ7ly1fn2lrbirtvj30mc01ndfu.jpg)

![](https://ws1.sinaimg.cn/large/0067fiZ7ly1fn2lrbn4h5j30nh01oaa3.jpg)

划分后的信息熵为

![](http://images.cnitblog.com/blog/571227/201412/121424414622037.png)

![](http://images.cnitblog.com/blog/571227/201412/121425239464583.png) 代表在特征属性 `T` 的条件下样本的**条件熵**。那么最终得到特征属性 `T` 带来的信息增益为

![](http://images.cnitblog.com/blog/571227/201412/121433352752920.png)


综上，信息增益的计算公式如下

![](http://images.cnitblog.com/blog/571227/201412/121628452909401.png)

其中 `S` 为全部样本集合，`value(T)` 是属性 `T` 所有取值的集合，`v` 是 `T` 的其中一个属性值，`Sv` 是 `S` 中属性 `T` 的值为 `v` 的样例集合，`|Sv|` 为 `Sv` 中所含样例数

在决策树的每一个非叶子结点划分之前，先计算每一个属性所带来的信息增益，选择最大信息增益的属性来划分，因为信息增益越大，区分样本的能力就越强，越具有代表性很显然这是一种自顶向下的贪心策略。以上就是ID3算法的核心思想。

## 决策树小结

如果决策树匹配的选项过多，则将这种问题称之为 **过度匹配(overfitting)**。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子结点。如果叶子结点只能增加少许信息，则可以删除该节点，将它并入到其他叶子结点中。

决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首先需要测量数据集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，知道数据集中的所有数据属于同一分类。ID3算法可以用于划分标称型数据集。构建决策树时，我们通常采用递归的方法将数据集转化为决策树。我们采用 python 内嵌的数据结构字典存储树节点信息。
